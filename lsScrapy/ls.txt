一、安装scrapy的两种方式：
1、在Python路径下安装，（比较麻烦，需要安装的包比较多）
2、在anaconda 下安装conda install scrapy即可（推荐）


二、初始化项目后，各个文件的作用：
1. 在spiders文件夹下编写自己的爬虫
2. 在items中编写容器用于存放爬取到的数据
3. 在pipelines中对数据进行各种操作
4. 在settings中进行项目的各种设置。


三、对scrapy经典框架爬虫原理的理解
1，spider打开某网页，获取到一个或者多个request，经由scrapy engine传送给调度器scheduler
request特别多并且速度特别快会在scheduler形成请求队列queue，由scheduler安排执行
2，schelduler会按照一定的次序取出请求，经由引擎, 下载器中间键，发送给下载器dowmloader
这里的下载器中间键是设定在请求执行前，因此可以设定代理，请求头，cookie等
3，下载下来的网页数据再次经过下载器中间键，经过引擎，经过爬虫中间键传送给爬虫spiders
这里的下载器中间键是设定在请求执行后，因此可以修改请求的结果
这里的爬虫中间键是设定在数据或者请求到达爬虫之前，与下载器中间键有类似的功能
4，由爬虫spider对下载下来的数据进行解析，按照item设定的数据结构经由爬虫中间键，引擎发送给项目管道itempipeline
这里的项目管道itempipeline可以对数据进行进一步的清洗，存储等操作
这里爬虫极有可能从数据中解析到进一步的请求request，它会把请求经由引擎重新发送给调度器shelduler，调度器循环执行上述操作
5，项目管道itempipeline管理着最后的输出





data flow
数据的flow,由执行的engine控制的，如图的各个步骤。
* Step1：scrapy engine先获取初始化的URL(Requests)。
* Step2: 然后交给Scheduler调度Requests，并且请求下一个URL.
* Step3: Scheduler返回下一个Requests给scrapy engine
* Step4: scrapy engine通过 Downloader Middlewares 将requests发送给下载器 Downloader (process_request()).
* Step5: 一旦Downloader Middlewares完成一个网页的下载之后，会产生一个响应Response，并且会将它通过Downloader Middlewares 发送回scrapy engine (process_response())。
* Step6: scrapy engine在接受到Response之后，通过Spider Middlewares会将它发送给Spider做进一步的处理。
* Step7: Spider处理完Response后会返回 scraped items and new Requests（下一页的链接）给scrapy engine。
* Step8: crapy engine 将处理后的items 发送给 Item Pipelines，然后把Requests会被传回Scheduler，并且请求下一个可能的URL进行爬取。
* Step9：以上从step1重复执行，直到在 Scheduler没有多余的URL进行请求

HINTS: 解释一下step7
Spider分析出来的结果有两种：
* 一种是需要进一步抓取的链接， 如 “下一页” 的链接， 它们会被传回Scheduler；
* 另一种是需要保存的数据， 它们被送到Item Pipeline里， 进行后期处理（详细分析、 过滤、 存储等）。


四、爬取流程
先初始化请求URL列表， 并指定下载后处理response的回调函数。
在parse回调中解析response并返回字典,Item对象,Request对象或它们的迭代对象。
在回调函数里面， 使用选择器解析页面内容， 并生成解析后的结果Item。
最后返回的这些Item通常会被持久化到数据库中(使用Item Pipeline)或者使用Feed exports将其保存到文件中


















运行方式：
1、切换到工程目录下直接运行
scrapy crawl 爬虫名


2、在主目录下建一个Python文件
内容：
from scrapy import cmdline
cmdline.execute(['scrapy','crawl','quotes'])




注意事项：spiders中的文件不能到下划线_,不然运行不了




