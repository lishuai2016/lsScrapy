安装scrapy的两种方式：
1、在Python路径下安装，（比较麻烦，需要安装的包比较多）
2、在anaconda 下安装conda install scrapy即可（推荐）


初始化项目后，各个文件的作用：
1. 在spiders文件夹下编写自己的爬虫
2. 在items中编写容器用于存放爬取到的数据
3. 在pipelines中对数据进行各种操作
4. 在settings中进行项目的各种设置。




对scrapy经典框架爬虫原理的理解
1，spider打开某网页，获取到一个或者多个request，经由scrapy engine传送给调度器scheduler
request特别多并且速度特别快会在scheduler形成请求队列queue，由scheduler安排执行
2，schelduler会按照一定的次序取出请求，经由引擎, 下载器中间键，发送给下载器dowmloader
这里的下载器中间键是设定在请求执行前，因此可以设定代理，请求头，cookie等
3，下载下来的网页数据再次经过下载器中间键，经过引擎，经过爬虫中间键传送给爬虫spiders
这里的下载器中间键是设定在请求执行后，因此可以修改请求的结果
这里的爬虫中间键是设定在数据或者请求到达爬虫之前，与下载器中间键有类似的功能
4，由爬虫spider对下载下来的数据进行解析，按照item设定的数据结构经由爬虫中间键，引擎发送给项目管道itempipeline
这里的项目管道itempipeline可以对数据进行进一步的清洗，存储等操作
这里爬虫极有可能从数据中解析到进一步的请求request，它会把请求经由引擎重新发送给调度器shelduler，调度器循环执行上述操作
5，项目管道itempipeline管理着最后的输出



运行方式：
1、切换到工程目录下直接运行
scrapy crawl 爬虫名


2、在主目录下建一个Python文件
内容：
from scrapy import cmdline
cmdline.execute(['scrapy','crawl','quotes'])




注意事项：spiders中的文件不能到下划线_,不然运行不了




